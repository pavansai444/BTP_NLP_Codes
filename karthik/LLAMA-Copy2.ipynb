{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bccb461-f7a8-460f-a3b3-2ebcdd67d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9619da1-c33d-4fda-9113-e77ec93857cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff66e03-2a82-4067-bd11-637986e2a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36394dba-12a0-49a6-b6db-fa225662de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ee0233-3f72-41d7-8028-0041fff779e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac441cb3-86f0-41b7-825e-139f57fa2035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b976f8d-8a83-4cdb-9b41-9c558cf52fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de63545c-8b7b-49f8-b1d1-8bd6de3dea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_eval = []\n",
    "y_eval = []\n",
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9405e183-3c43-4ab8-9196-072585481a7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      2\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2-9b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3279\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3276\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3279\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3282\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3283\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_id = \"google/gemma-2-9b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "max_seq_length = 2048\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\", max_seq_length=max_seq_length)\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e0c78d-6624-4b71-a25c-6479d3960a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(text,lab):\n",
    "    return f\"\"\"generate_prompt\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n",
    "            [{text}] = {lab}\n",
    "            \"\"\".strip() + EOS_TOKEN\n",
    "\n",
    "def generate_test_prompt(text):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n",
    "            [{text}] = \"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "337b6b6f-b1d4-481f-b2e2-e2bf9df79726",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Enhindata/train.txt',encoding='UTF-8') as rf:\n",
    "    lines = rf.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a078302d-5047-44dc-b28a-5bd7bfb31d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for lin in lines:\n",
    "    da = lin.split('\\t')\n",
    "    lab = da[1].strip()\n",
    "    X_train.append(da[0])\n",
    "    y_train.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "627a3731-4681-4bcf-9e34-f516265626b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000,\n",
       " 'nen vist bolest vztek smutek zmatek osam lost beznad j a nakonec jen klid Asi takhle vypad m j life')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "713b89a3-3783-4d18-867f-83efafed9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Enhindata/validation.txt',encoding='UTF-8') as rf:\n",
    "    lines = rf.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "462117f7-02f4-4bd5-bc5e-33b042f22c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lin in lines:\n",
    "    da = lin.split('\\t')\n",
    "    lab = da[1].strip()\n",
    "    X_eval.append(da[0])\n",
    "    y_eval.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4123e86e-9cf9-4068-8166-b33930a14eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,\n",
       " 'prahladspatel modi mantrimandal may samil honay par badhai narmaday har')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_eval),X_eval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f540793a-9d8a-4d62-a210-b3d2c52e5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Enhindata/test.txt',encoding='UTF-8') as rf:\n",
    "    lines = rf.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8af625bb-bf20-4983-b661-d023a0ec44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]\n",
    "y_test = []\n",
    "for lin in lines:\n",
    "    da = lin.split('\\t')\n",
    "    lab = da[1].strip()\n",
    "    X_test.append(da[0])\n",
    "    y_test.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7535f8-38cf-49d6-9a2f-ff531eb9ab0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc8bebae-3077-4978-9d51-c561a6360789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bd828d6-a1bc-43ea-a5d1-ef84f461fa1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EOS_TOKEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n\u001b[0;32m----> 2\u001b[0m     train_data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mgenerate_prompt\u001b[0;34m(text, lab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(text,lab):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mgenerate_prompt\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m            Analyze the sentiment of the news headline enclosed in square brackets, \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m            determine if it is positive, neutral, or negative, and return the answer as \u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m            the corresponding sentiment label \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m            [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlab\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m+\u001b[39m \u001b[43mEOS_TOKEN\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EOS_TOKEN' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_train)):\n",
    "    train_data.append(generate_prompt(X_train[i],y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98748421-df41-43df-b40c-d0216838e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33c58d53-d0ba-49cd-b364-e8dfbea2b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fffc2-d7e7-4bf6-9355-d8a969a043a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8feed73d-a37e-469b-9828-724a261672ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "for i in range(len(X_eval)):\n",
    "    eval_data.append(generate_prompt(X_eval[i],y_eval[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec9e7334-dd8b-4be4-9d37-a154cc90c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(eval_data, columns=['text'])\n",
    "eval_data = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1975a866-54e9-423b-a6de-9f7140730d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4745cc87-cbf7-484f-b2a2-4f0578624e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append(generate_test_prompt(X_test[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf6f7490-6a43-4409-89b8-44a8ed9d0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_data, columns=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "939297ce-b0a3-4595-8853-fbb34c175aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "922b9706-d941-41fb-b57a-f12996e83c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f181c797-edaf-44a1-9281-1df2ac9eca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb432033-af2f-43a3-9447-94803bf27155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9adb1116e244b9be3f711b4ba55d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e6af8f3071438d9a050af8d9e3a17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True,\n",
    "                                         )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfa7da0f-0abd-468e-a483-d247115cedca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832eab0a6fde4f919e2d8deb861f3ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883ed0efbed7464e9592d59d6bb3d9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"trained_weigths\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16, \n",
    "        lora_dropout=0.05,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        target_modules =[\"q_proj\", \"v_proj\", \"k_proj\", \"down_proj\", \"gate_proj\", \"up_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=3,                       # number of training epochs\n",
    "    per_device_train_batch_size=16,            # batch size per device during training\n",
    "    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,                         # log every 10 steps\n",
    "    learning_rate=5e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    report_to=\"tensorboard\",                  # report metrics to tensorboard\n",
    "    evaluation_strategy=\"epoch\"               # save checkpoint every epoch\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0586b48-bae7-4f6f-b3be-18297617ebe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 56:46, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.634000</td>\n",
       "      <td>1.705342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.523600</td>\n",
       "      <td>1.593814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.466100</td>\n",
       "      <td>1.582825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=327, training_loss=1.589648884006232, metrics={'train_runtime': 3416.2173, 'train_samples_per_second': 12.294, 'train_steps_per_second': 0.096, 'total_flos': 1.7919495695499264e+17, 'train_loss': 1.589648884006232, 'epoch': 2.99})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b060f02-b6fe-4f87-942a-8f13e76ea414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_weigths/tokenizer_config.json',\n",
       " 'trained_weigths/special_tokens_map.json',\n",
       " 'trained_weigths/tokenizer.model',\n",
       " 'trained_weigths/added_tokens.json',\n",
       " 'trained_weigths/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "550197fa-57d5-4440-8513-61c1a48d9bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa55acda59c4987a1fe9efe4ce4e167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./merged_model/tokenizer_config.json',\n",
       " './merged_model/special_tokens_map.json',\n",
       " './merged_model/tokenizer.model',\n",
       " './merged_model/added_tokens.json',\n",
       " './merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "finetuned_model = \"./trained_weigths/\"\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "     finetuned_model,\n",
    "     torch_dtype=compute_dtype,\n",
    "     return_dict=False,\n",
    "     low_cpu_mem_usage=True,\n",
    "     device_map=device,\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\n",
    "tokenizer.save_pretrained(\"./merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458244b-607f-4686-9a92-4e056cc1f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = predict(test_df, merged_model, tokenizer)\n",
    "# evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a6244c-a8ce-4cb3-9f29-fcf5c8068424",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15eb83-c206-4822-8b20-d8553c7786d7",
   "metadata": {},
   "outputs": [],
   "source": [
    " y_pred = []\n",
    "# for i in tqdm(range(len(test_data))):\n",
    "#     prompt = test_data[i]\n",
    "#     pipe = pipeline(task=\"text-generation\", \n",
    "#                         model=merged_model, \n",
    "#                         tokenizer=tokenizer, \n",
    "#                         max_new_tokens = 1, \n",
    "#                         temperature = 0.0,\n",
    "#                        )\n",
    "#     result = pipe(prompt)\n",
    "#     answer = result[0]['generated_text'].split(\"=\")[-1]\n",
    "#     if \"positive\" in answer:\n",
    "#         y_pred.append(\"positive\")\n",
    "#     elif \"negative\" in answer:\n",
    "#          y_pred.append(\"negative\")\n",
    "#     else:\n",
    "#         y_pred.append(\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea15843-3877-4780-ba18-780e10f89ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9773d3c-0e7d-4e60-813f-49bb4c59be8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cfe5d1a-1b63-46b8-912e-b5feca1f62d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0cad94f-1b1c-430c-868b-5f6e24fd31a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3000/3000 [01:38<00:00, 30.51it/s]\n"
     ]
    }
   ],
   "source": [
    " y_pred = []\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    prompt = test_data[i]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    max_new_tokens = 1\n",
    "    temperature = 0.0001\n",
    "    output = merged_model.generate(input_ids=inputs.input_ids, \n",
    "                               max_length=len(inputs.input_ids[0]) + max_new_tokens, \n",
    "                               temperature=temperature, \n",
    "                               pad_token_id=tokenizer.eos_token_id,\n",
    "                               eos_token_id=tokenizer.pad_token_id)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    answer = generated_text.split(\"=\")[-1]\n",
    "    if \"positive\" in answer:\n",
    "       y_pred.append(\"positive\")\n",
    "    elif \"negative\" in answer:\n",
    "       y_pred.append(\"negative\")\n",
    "    else:\n",
    "       y_pred.append(\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e64888b-f6e9-4b78-aa59-06d4914aefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "weighted_f1 = f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e057e21c-b346-42b6-a047-d02abfc0e92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7252620851284662"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabdfe2-ea0b-4cd4-8165-69c18721552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"sa_spa_eng\", \"w\") as writer:\n",
    "#         writer.write('\\n'.join(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7f252-78fc-4d3e-a16e-b6fe4787cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83560c4c-a4f9-4f9f-a7d5-ffba3df4a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca47823-5896-4731-a44b-a55f26a4acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d5117-6a58-47f2-9d82-6575e8246348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
